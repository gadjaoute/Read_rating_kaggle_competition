{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import urllib \n",
    "import scipy.optimize \n",
    "import random\n",
    "import gzip\n",
    "import ast\n",
    "import csv\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with gzip.open(\"assignment1.tar.gz\", \"rb\") as f:\n",
    "    file_content=f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tarfile\n",
    "tar=tarfile.open(\"assignment1.tar.gz\")\n",
    "tar.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for member in tar.getmembers():\n",
    "    print(\"Extracting %s\" % member.name)\n",
    "    tar.extract(member)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "path = \"train_Interactions.csv.gz\"\n",
    "\n",
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)\n",
    "\n",
    "def readCSV(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        yield l.strip().split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Although we have built a validation set, it only consists of positive samples. For this task we also need\n",
    "examples of user/item pairs that weren’t read. For each entry (user,book) in the validation set, sample a\n",
    "negative entry by randomly choosing a book that user hasn’t read.1 Evaluate the performance (accuracy)\n",
    "of the baseline model on the validation set you have built (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "df = pd.read_csv('train_Interactions.csv')\n",
    "for i in range(len(df)):\n",
    "    X.extend([[df['userID'][i], df['bookID'][i], df['rating'][i]]])\n",
    "    y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy=list(zip(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[0:190000]\n",
    "y_train = y[0:190000]\n",
    "X_val = X[190000:200000]\n",
    "y_val = y[190000:200000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u93625722', 'b90140601', 4]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict(set)\n",
    "itemsPerUser = defaultdict(set)\n",
    "all_books = set([x[1] for x in X])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each (user, book) in the train set, randomly choose a book from the entire dataset. If the random book is not in booksPerUser[user], add that (user, random_book) pair to the train set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2=[]\n",
    "\n",
    "for user_book in X_train:\n",
    "    current_user = user_book[0]\n",
    "    #rating=-1\n",
    "    rating=0\n",
    "    the_diff=list(all_books.difference(itemsPerUser[current_user]))\n",
    "    rand_idx = random.randrange(len(the_diff)) \n",
    "    choice = the_diff[rand_idx]\n",
    "    X_train2.append([current_user, choice,rating])\n",
    "    y_train.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.extend(X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) For each (user, book) in the validation set, randomly choose a book from the entire dataset. If the random book is not in booksPerUser[user], add that (user, random_book) pair to the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val2=[]\n",
    "\n",
    "for user_book in X_val:\n",
    "    current_user = user_book[0]\n",
    "    #rating=-1\n",
    "    rating=0\n",
    "    the_diff=list(all_books.difference(itemsPerUser[current_user]))\n",
    "    rand_idx = random.randrange(len(the_diff)) \n",
    "    choice = the_diff[rand_idx]\n",
    "    X_val2.append([current_user, choice,rating])\n",
    "    y_val.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.extend(X_val2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) I use the provided baseline code to compute the baseline's predictions on the training set. \n",
    "This returns a set of the most popular 50% of books based on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for user,book,rating in X_train: #running baseline code on the training set I created for read prediction.\n",
    "    bookCount[book] += 1\n",
    "    totalRead += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalRead/2: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) For each (user, book) in the new balanced validation set, I compute the model's prediction using the previous set (which was trained on only the training set). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val_predictions=[]\n",
    "\n",
    "for l in range(len(X_val)):\n",
    "    u=X_val[l][0]\n",
    "    b=X_val[l][1]\n",
    "    if b in return1:\n",
    "        Y_val_predictions.append(1)\n",
    "    else:\n",
    "        Y_val_predictions.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val_actual=[]\n",
    "\n",
    "for l in range(len(X_val)):\n",
    "    #continue \n",
    "    u=X_val[l][0]\n",
    "    b=X_val[l][1]\n",
    "    rating=X_val[l][2]\n",
    "    \n",
    "    if rating == 0:\n",
    "        Y_val_actual.append(0) #code this as the book was not read\n",
    "    else:\n",
    "        Y_val_actual.append(1) #code this as the book was read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_val_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of baseline classifier on the validation set is: 0.64075\n",
      "BER is: 0.3598249957897621\n",
      "F1 is: 0.6251760655224582\n",
      "F10 is: 0.6273252909140288\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "tn,fp,fn,tp = confusion_matrix(Y_val_actual,Y_val_predictions).ravel()\n",
    "accuracy=(tp+tn)/(tp+tn+fp+fn)\n",
    "TNR=tn/(tn+fp)\n",
    "TPR=tp/(tp+fn)\n",
    "BER=1-0.5*(TPR+TNR)\n",
    "recall=tp/(tp+fn)\n",
    "precision=tp/(tp+fp)\n",
    "F1=2*(precision*recall)/(precision+recall)\n",
    "F10=101*(precision*recall)/(100*precision+recall)\n",
    "\n",
    "print(\"Accuracy of baseline classifier on the validation set is:\",accuracy)\n",
    "print(\"BER is:\",BER)\n",
    "print(\"F1 is:\",F1)\n",
    "print(\"F10 is:\",F10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "Try other thresholds or percentiles for popularity, to come up with better accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9] \n",
    "accuracy_per_threshold=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.58185\n",
      "0.2\n",
      "0.6212\n",
      "0.3\n",
      "0.643\n",
      "0.4\n",
      "0.65025\n",
      "0.5\n",
      "0.64075\n",
      "0.6\n",
      "0.6216\n",
      "0.7\n",
      "0.59565\n",
      "0.8\n",
      "0.5654\n",
      "0.9\n",
      "0.52635\n"
     ]
    }
   ],
   "source": [
    "for tr in threshold:\n",
    "    print(tr)\n",
    "    bookCount2= defaultdict(int)\n",
    "    totalRead2 = 0\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        user2=X_train[i][0]\n",
    "        book2=X_train[i][1]\n",
    "        bookCount2[book2] += 1\n",
    "        totalRead2 += 1\n",
    "    \n",
    "    mostPopular2 = [(bookCount2[x], x) for x in bookCount2]\n",
    "    mostPopular2.sort()\n",
    "    mostPopular2.reverse()\n",
    "    \n",
    "    return2 = set()\n",
    "    count2 = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count2 += ic\n",
    "        return2.add(i)\n",
    "        if count2 > totalRead2*tr: break\n",
    "    \n",
    "    Y_val_predictions2=[]\n",
    "    \n",
    "    for l in range(len(X_val)):\n",
    "    #continue \n",
    "        u=X_val[l][0]\n",
    "        b=X_val[l][1]\n",
    "        if b in return2:\n",
    "            Y_val_predictions2.append(1)\n",
    "        else:\n",
    "            Y_val_predictions2.append(0)\n",
    "            \n",
    "    tn2,fp2,fn2,tp2 = confusion_matrix(Y_val_actual,Y_val_predictions2).ravel()\n",
    "    accuracy2=(tp2+tn2)/(tp2+tn2+fp2+fn2)\n",
    "    print(accuracy2)\n",
    "    accuracy_per_threshold.append(accuracy2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph accuracy for different Thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(threshold,accuracy_per_threshold, color=\"turquoise\",label=\"Accuracy for Valid set\")\n",
    "plt.suptitle(\"Accuracy per threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the graph, we can conclude that a threshold of 0.4 rather than 0.5 will increase the accuracy of the baseline model on the validation set. If we print the accuracy with further threshold we find that 0.589 has the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df=pd.DataFrame(X_train, columns = ['userID' , 'bookID', 'rating']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>bookID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>u79354815</td>\n",
       "      <td>b14275065</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>u56917948</td>\n",
       "      <td>b82152306</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>u97915914</td>\n",
       "      <td>b44882292</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>u49688858</td>\n",
       "      <td>b79927466</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>u08384938</td>\n",
       "      <td>b05683889</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      userID     bookID  rating\n",
       "0  u79354815  b14275065       4\n",
       "1  u56917948  b82152306       5\n",
       "2  u97915914  b44882292       5\n",
       "3  u49688858  b79927466       5\n",
       "4  u08384938  b05683889       2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_df=pd.DataFrame(X_val, columns = ['userID' , 'bookID', 'rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=[0.4] #this is the threshold we select for the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n",
      "0.65025\n"
     ]
    }
   ],
   "source": [
    "#Build Popularity Feature:\n",
    "for tr in threshold:\n",
    "    print(tr)\n",
    "    bookCount2= defaultdict(int)\n",
    "    totalRead2 = 0\n",
    "\n",
    "    for i in range(len(X_train_df)):\n",
    "        user2=X_train_df.iloc[i][0]\n",
    "        book2=X_train_df.iloc[i][1]\n",
    "        bookCount2[book2] += 1\n",
    "        totalRead2 += 1\n",
    "    \n",
    "    mostPopular2 = [(bookCount2[x], x) for x in bookCount2]\n",
    "    mostPopular2.sort()\n",
    "    mostPopular2.reverse()\n",
    "    \n",
    "    return2 = set()\n",
    "    count2 = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count2 += ic\n",
    "        return2.add(i)\n",
    "        if count2 > totalRead2*tr: break\n",
    "    \n",
    "    Y_predictions_pop=[]\n",
    "    \n",
    "    for l in range(len(X_val_df)): \n",
    "        u=X_val_df.iloc[l][0]\n",
    "        b=X_val_df.iloc[l][1]\n",
    "        if b in return2:\n",
    "            Y_predictions_pop.append(1)\n",
    "        else:\n",
    "            Y_predictions_pop.append(0)\n",
    "            \n",
    "    tn2,fp2,fn2,tp2 = confusion_matrix(Y_val_actual,Y_predictions_pop).ravel()\n",
    "    accuracy2=(tp2+tn2)/(tp2+tn2+fp2+fn2)\n",
    "    print(accuracy2)\n",
    "    accuracy_per_threshold.append(accuracy2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***3.*** A stronger baseline than the one provided might make use of the Jaccard similarity (or another similarity metric). Given a pair (u, b) in the validation set, consider all training items b' that user u has read. For each, compute the Jaccard similarity between b and b', i.e., users (in the training set) who have read b and users who have read b'. Predict as ‘read’ if the maximum of these Jaccard similarities exceeds a threshold (you may choose the threshold that works best). Report the performance on your validation set (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT I DID IN HMW 3\n",
    "# Obtain all available books in the train set:\n",
    "allBooks_train = X_train_df['bookID'].unique().tolist()\n",
    "# Get Books per user on the train set:\n",
    "book_user_train = X_train_df.groupby('userID')\n",
    "#Users per book on the train set\n",
    "user_per_book_train = X_train_df.groupby('bookID')\n",
    "\n",
    "y_predictions_jaccard = []\n",
    "similar = []\n",
    "threshold = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_val_df)):\n",
    "    user = X_val_df.iloc[i][0]\n",
    "    BK = X_val_df.iloc[i][1]\n",
    "    \n",
    "    sim = []\n",
    "    if BK in allBooks_train:\n",
    "        BK_users = set(user_per_book_train.get_group(BK)['userID'])\n",
    "        Book_read = book_user_train.get_group(user)['bookID']\n",
    "             \n",
    "        if len(BK_users) == 1:\n",
    "            users_book_read = {}\n",
    "            sim.append(Jaccard(BK_users,users_book_read))\n",
    "            if max(sim) > threshold:\n",
    "                y_predictions_jaccard.append(1)\n",
    "            else:\n",
    "                y_predictions_jaccard.append(0)      \n",
    "        else:\n",
    "            for i in Book_read:\n",
    "                users_book_read = set(user_per_book_train.get_group(i)['userID'])\n",
    "                sim.append(Jaccard(BK_users,users_book_read))\n",
    "                \n",
    "            if max(sim) > threshold:\n",
    "                y_predictions_jaccard.append(1)\n",
    "            else:\n",
    "                y_predictions_jaccard.append(0)\n",
    "    else: \n",
    "        sim = 0\n",
    "        y_predictions_jaccard.append(0)\n",
    "    \n",
    "    similar.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_predictions_jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4788"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn4,fp4,fn4,tp4= confusion_matrix(Y_val_actual,y_predictions_jaccard).ravel()\n",
    "accuracy4=(tp4+tn4)/(tp4+tn4+fp4+fn4)\n",
    "#the accuracy is: \n",
    "accuracy4 #0.5928"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of combining the model as I did in homework #3, I decided to use ensembling. I build a feature vector using our predictions from the popularity predictions (using a threshold of 0.4) as well as the jaccard predictions (using a threshold of 0.00001). I then run a logistic regression on both features to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.column_stack((Y_predictions_pop, y_predictions_jaccard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_predictions_jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=Y_val_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = linear_model.LogisticRegression(C=0.0001, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.0001, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.fit(features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions from the data\n",
    "predictions = mod.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False, ...,  True,  True, False])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check whether they match the labels\n",
    "correctPredictions = predictions == y\n",
    "correctPredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for regression is = \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.65025"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Accuracy for regression is = ')\n",
    "sum(predictions == y) / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the best model: LogisticRegression(C=0.0001, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "What is the score of the best model: 0.65025\n",
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
      "0       0.026480      0.002532         0.003636        0.001391  0.0001   \n",
      "1       0.022378      0.000031         0.002651        0.000087   0.001   \n",
      "2       0.024041      0.000980         0.002610        0.000044    0.01   \n",
      "3       0.024641      0.000248         0.002580        0.000018     0.1   \n",
      "4       0.024645      0.000155         0.002614        0.000026       1   \n",
      "5       0.025426      0.000938         0.002634        0.000042      10   \n",
      "6       0.027855      0.006695         0.002992        0.000723     100   \n",
      "7       0.029888      0.006221         0.002998        0.000748    1000   \n",
      "\n",
      "          params  split0_test_score  split1_test_score  split2_test_score  \\\n",
      "0  {'C': 0.0001}           0.628593             0.6555            0.64075   \n",
      "1   {'C': 0.001}           0.628593             0.6555            0.64075   \n",
      "2    {'C': 0.01}           0.628593             0.6555            0.64075   \n",
      "3     {'C': 0.1}           0.628593             0.6555            0.64075   \n",
      "4       {'C': 1}           0.628593             0.6555            0.64075   \n",
      "5      {'C': 10}           0.628593             0.6555            0.64075   \n",
      "6     {'C': 100}           0.628593             0.6555            0.64075   \n",
      "7    {'C': 1000}           0.628593             0.6555            0.64075   \n",
      "\n",
      "   split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
      "0              0.657           0.669417          0.65025        0.014141   \n",
      "1              0.657           0.669417          0.65025        0.014141   \n",
      "2              0.657           0.669417          0.65025        0.014141   \n",
      "3              0.657           0.669417          0.65025        0.014141   \n",
      "4              0.657           0.669417          0.65025        0.014141   \n",
      "5              0.657           0.669167          0.65020        0.014073   \n",
      "6              0.657           0.669167          0.65020        0.014073   \n",
      "7              0.657           0.669167          0.65020        0.014073   \n",
      "\n",
      "   rank_test_score  \n",
      "0                1  \n",
      "1                1  \n",
      "2                1  \n",
      "3                1  \n",
      "4                1  \n",
      "5                6  \n",
      "6                6  \n",
      "7                6  \n"
     ]
    }
   ],
   "source": [
    "#used to calculate C value above:\n",
    "\n",
    "lr = linear_model.LogisticRegression()\n",
    "parameters = {'C':[0.0001,0.001,.01,0.1,1,10,100,1000]}\n",
    "clf = GridSearchCV(lr, parameters, cv=5) #run 5 different experiment on 5 parameters\n",
    "model_grid= clf.fit(features, y)\n",
    "print('What is the best model:', clf.best_estimator_)  \n",
    "print('What is the score of the best model:',clf.best_score_) \n",
    "results_grid = pd.DataFrame(clf.cv_results_)\n",
    "print(results_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.fit(features, y)\n",
    "predict_test=mod.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = []\n",
    "users = []\n",
    "X5 = []\n",
    "y5 = []\n",
    "df5 = pd.read_csv('pairs_Read.txt', sep =\"-\" )\n",
    "df5.rename(columns = {'bookID,prediction':'bookID'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, l in enumerate(open('pairs_Read.txt')):\n",
    "    if l.startswith(\"userID\"):\n",
    "        with open('Read_predictions_assignment1.csv', 'w') as csvFile:\n",
    "            writer = csv.writer(csvFile)\n",
    "            writer.writerow([\"userID-bookID\",\"prediction\"])\n",
    "        continue\n",
    "  u,b = l.strip().split('-')\n",
    "  if b in not in books:\n",
    "    predictions.write(u + '-' + b + \",1\\n\")\n",
    "  else:\n",
    "    predictions.write(u + '-' + b + \",0\\n\")\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Read.txt\", 'w')\n",
    "for i in range(len(df5)):\n",
    "    user = df5.iloc[i][0]\n",
    "    BK = df5.iloc[i][1]\n",
    "    if BK in return4:\n",
    "        sim = []\n",
    "        if BK in allBooks_train:\n",
    "            BK_users = set(user_per_book_train.get_group(BK)['userID'])\n",
    "            Book_read = book_user_train.get_group(user)['bookID']\n",
    "        if len(BK_users) == 1:\n",
    "            users_book_read = {}\n",
    "            sim.append(Jaccard(BK_users,users_book_read))\n",
    "            if max(sim) > threshold4:\n",
    "                predictions.write(user + '-' + BK + \",1\\n\")\n",
    "            else:\n",
    "                predictions.write(user + '-' + BK + \",0\\n\") \n",
    "        else:\n",
    "            # Look for users (in the train set) who have read BK and users who have read Book_read\n",
    "            for i in Book_read:\n",
    "                users_book_read = set(user_per_book_train.get_group(i)['userID'])\n",
    "                sim.append(Jaccard(BK_users,users_book_read))\n",
    "            \n",
    "            if max(sim) > threshold4:\n",
    "                predictions.write(user + '-' + BK + \",1\\n\")\n",
    "            else:\n",
    "                predictions.write(user + '-' + BK + \",0\\n\")\n",
    "    else: \n",
    "            sim = 0\n",
    "            predictions.write(user + '-' + BK + \",0\\n\")\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***4.*** Improve the above predictor by incorporating both a Jaccard-based threshold and a popularity based\n",
    "threshold. Report the performance on your validation set (1 mark). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Threshold=[0.4] # this is the most effective Threshold from above, but we find later that using 0.6 gives a better performance on kaggle.\n",
    "Y_val_predictions4=[]\n",
    "similar4 = []\n",
    "threshold4 = 0.000001\n",
    "sim=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logic for algorithm: Using Jaccard and Popular Method\n",
    "if popular book> 50% and  if sim>0.1,\n",
    "then append 1, else append 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tr in Threshold:\n",
    "    print(tr)\n",
    "    bookCount4= defaultdict(int)\n",
    "    totalRead4 = 0\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "            user4=X_train[i][0]\n",
    "            book4=X_train[i][1]\n",
    "            bookCount4[book4] += 1\n",
    "            totalRead4 += 1\n",
    "    \n",
    "    mostPopular4 = [(bookCount4[x], x) for x in bookCount4]\n",
    "    mostPopular4.sort()\n",
    "    mostPopular4.reverse()\n",
    "    \n",
    "    return4 = set()\n",
    "    count4 = 0\n",
    "    \n",
    "    for ic, i in mostPopular:\n",
    "        count4 += ic\n",
    "        return4.add(i)\n",
    "        if count4 > totalRead4*tr: break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining Models:\n",
    "Y_val_predictions4=[]\n",
    "similar4 = []\n",
    "threshold4 = 0.000001\n",
    "\n",
    "#1\n",
    "for i in range(len(X_val_df)):\n",
    "    #print(k)\n",
    "    user = X_val_df.iloc[i][0]\n",
    "    BK = X_val_df.iloc[i][1]\n",
    "    \n",
    "    if BK in return4:\n",
    "        sim = []\n",
    "        \n",
    "        if BK in allBooks_train:\n",
    "            BK_users = set(user_per_book_train.get_group(BK)['userID'])\n",
    "        \n",
    "            Book_read = book_user_train.get_group(user)['bookID']\n",
    "        \n",
    "        if len(BK_users) == 1:\n",
    "            users_book_read = {}\n",
    "            sim.append(Jaccard(BK_users,users_book_read))\n",
    "            if max(sim) > threshold4:\n",
    "                Y_val_predictions4.append(1)\n",
    "            else:\n",
    "                Y_val_predictions4.append(0) \n",
    "              \n",
    "        else:\n",
    "            for i in Book_read:\n",
    "               \n",
    "                users_book_read = set(user_per_book_train.get_group(i)['userID'])\n",
    "                sim.append(Jaccard(BK_users,users_book_read))\n",
    "            \n",
    "            if max(sim) > threshold:\n",
    "               \n",
    "            #if max(sim) > threshold4:\n",
    "                Y_val_predictions4.append(1)\n",
    "            else:\n",
    "                Y_val_predictions4.append(0)\n",
    "    else: \n",
    "            sim = 0\n",
    "            Y_val_predictions4.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn5,fp5,fn5,tp5= confusion_matrix(Y_val_actual,Y_val_predictions4).ravel()\n",
    "accuracy5=(tp5+tn5)/(tp5+tn5+fp5+fn5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(Y_val_actual,Y_val_predictions4)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***5.*** To run our model on the test set, we’ll have to use the files ‘pairs Read.txt’ to find the reviewerID/itemID\n",
    "pairs about which we have to make predictions. Using that data, run the above model and upload your\n",
    "solution to Kaggle. Tell us your Kaggle user name (1 mark). If you’ve already uploaded a better solution\n",
    "to Kaggle, that’s fine too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You are supposed to read in the pairs_Read.txt file, make predictions for each of the 20,000 rows in this file \n",
    "#and write out the predictions to a new predictions_Read.txt file. \n",
    "#The code snippet in the answer shows you how to do this. Finally submit the predictions_Read.txt file to Kaggle.\n",
    "Threshold=[0.4]\n",
    "Y_val_predictions4=[]\n",
    "similar4 = []\n",
    "threshold4 = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tr in Threshold:\n",
    "    print(tr)\n",
    "    bookCount4= defaultdict(int)\n",
    "    totalRead4 = 0\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "            user4=X_train[i][0]\n",
    "            book4=X_train[i][1]\n",
    "            bookCount4[book4] += 1\n",
    "            totalRead4 += 1\n",
    "    \n",
    "    mostPopular4 = [(bookCount4[x], x) for x in bookCount4]\n",
    "    mostPopular4.sort()\n",
    "    mostPopular4.reverse()\n",
    "    \n",
    "    return4 = set()\n",
    "    count4 = 0\n",
    "    \n",
    "    for ic, i in mostPopular:\n",
    "        count4 += ic\n",
    "        return4.add(i)\n",
    "        if count4 > totalRead4*tr: break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = []\n",
    "users = []\n",
    "X5 = []\n",
    "y5 = []\n",
    "df5 = pd.read_csv('pairs_Read.txt', sep =\"-\" )\n",
    "df5.rename(columns = {'bookID,prediction':'bookID'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Read.txt\", 'w')\n",
    "for i in range(len(df5)):\n",
    "    #u,b = df5.iloc[1][0].strip().split(\"-\")\n",
    "    #book5.append(b)\n",
    "   # users5.append(u)\n",
    "    user = df5.iloc[i][0]\n",
    "    BK = df5.iloc[i][1]\n",
    "    if BK in return4:\n",
    "        sim = []\n",
    "        if BK in allBooks_train:\n",
    "            BK_users = set(user_per_book_train.get_group(BK)['userID'])\n",
    "            Book_read = book_user_train.get_group(user)['bookID']\n",
    "        if len(BK_users) == 1:\n",
    "            users_book_read = {}\n",
    "            sim.append(Jaccard(BK_users,users_book_read))\n",
    "            if max(sim) > threshold4:\n",
    "                predictions.write(user + '-' + BK + \",1\\n\")\n",
    "            else:\n",
    "                predictions.write(user + '-' + BK + \",0\\n\") \n",
    "        else:\n",
    "            # Look for users (in the train set) who have read BK and users who have read Book_read\n",
    "            for i in Book_read:\n",
    "                users_book_read = set(user_per_book_train.get_group(i)['userID'])\n",
    "                sim.append(Jaccard(BK_users,users_book_read))\n",
    "            \n",
    "            if max(sim) > threshold4:\n",
    "                predictions.write(user + '-' + BK + \",1\\n\")\n",
    "            else:\n",
    "                predictions.write(user + '-' + BK + \",0\\n\")\n",
    "    else: \n",
    "            sim = 0\n",
    "            predictions.write(user + '-' + BK + \",0\\n\")\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "dframe = pd.read_csv('predictions_Read.txt', header=None, names=['userID-bookID', 'prediction'])\n",
    "dframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv=dframe.to_csv('predictions_Read.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KAGGLE USERNAME : Ghislene Adjaoute, score is 0.67116\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start by building our training/validation sets much as we did for the first task. This time building a\n",
    "validation set is more straightforward: you can simply use part of the data for validation, and do not need to\n",
    "randomly sample non-read users/books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. \n",
    "Fit a predictor of the form \n",
    "rating(user, item) 'α + βuser + βitem\n",
    "by fitting the mean and the two bias terms as described in the lecture notes. Use a regularization parameter of λ = 1. Report the MSE on the validation set (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_by_item =defaultdict(list)\n",
    "book_by_user= defaultdict(list) #how many people read each book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'train_interactions.csv.gz'\n",
    "f = gzip.open(path, 'rt', encoding=\"utf8\")\n",
    "header = f.readline()\n",
    "header = header.strip().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset9 = []\n",
    "for line in f:\n",
    "    fields = line.strip().split(',')\n",
    "    d = dict(zip(header, fields))\n",
    "    d['rating'] = int(d['rating'])\n",
    "    dataset9.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vvv = int(80*len(dataset9)/100)\n",
    "train = dataset9[:vvv]\n",
    "validation = dataset9[vvv:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerBook = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in train:\n",
    "    user,book = d['userID'], d['bookID']\n",
    "    ratingsPerUser[user].append(d)\n",
    "    ratingsPerBook[book].append(d)\n",
    "    rating = d['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingMean = sum([int(d['rating']) for d in train]) / len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(train)\n",
    "nUsers = len(ratingsPerUser)\n",
    "nItems = len(ratingsPerBook)\n",
    "users = list(ratingsPerUser.keys())\n",
    "items = list(ratingsPerBook.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "userBiases = defaultdict(float)\n",
    "itemBiases = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(user, item):\n",
    "    return alpha + userBiases[user] + itemBiases[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global itemBiases\n",
    "    alpha = theta[0]\n",
    "    userBiases = dict(zip(users, theta[1:nUsers+1]))\n",
    "    itemBiases = dict(zip(items, theta[1+nUsers:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(d['userID'], d['bookID']) for d in train]\n",
    "    cost = MSE(predictions, labels)\n",
    "    print(\"MSE = \" + str(cost))\n",
    "    for u in userBiases:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "    for i in itemBiases:\n",
    "        cost += lamb*itemBiases[i]**2\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(train)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    for d in train:\n",
    "        u,b = d['userID'], d['bookID']\n",
    "        pred = prediction(u, b)\n",
    "        diff = pred - int(d['rating'])\n",
    "        dalpha += 2/N*diff\n",
    "        dUserBiases[u] += 2/N*diff\n",
    "        dItemBiases[b] += 2/N*diff\n",
    "    for u in userBiases:\n",
    "        dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "    for b in itemBiases:\n",
    "        dItemBiases[b] += 2*lamb*itemBiases[b]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[b] for b in items]\n",
    "    return numpy.array(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "alwaysPredictMean = [ratingMean for d in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = [int(d['rating']) for d in train]\n",
    "labels_valid = [int(d['rating']) for d in validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4777248085935766"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(alwaysPredictMean, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 1.4777248085935766\n",
      "MSE = 1.4598930602062035\n",
      "MSE = 1.477560464109261\n",
      "MSE = 1.4775604617837361\n"
     ]
    }
   ],
   "source": [
    "result= scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nItems),\n",
    "                             derivative, args = (labels_train, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.896409859007681"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions Training set\n",
    "predictions_train = []\n",
    "for d in train:\n",
    "    user,book = d['userID'], d['bookID']\n",
    "    \n",
    "    if book in itemBiases and user in userBiases:\n",
    "         pred1 = alpha + userBiases[user] + itemBiases[book]\n",
    "    else:\n",
    "        pred1 = alpha\n",
    "    predictions_train.append(pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4775604617837361"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(predictions_train,labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat for predictions validation:\n",
    "\n",
    "predictions_validation = []\n",
    "for d in validation:\n",
    "    user,book = d['userID'], d['bookID']\n",
    "    \n",
    "    if book in itemBiases and user in userBiases:\n",
    "         pred1 = alpha + userBiases[user] + itemBiases[book]\n",
    "    else:\n",
    "        pred1 = alpha\n",
    "    predictions_validation.append(pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4610529908242351"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_Validation = MSE(predictions_validation,labels_valid)\n",
    "MSE_Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a better value of λ using your validation set. Report the value you chose, its MSE, and upload your\n",
    "solution to Kaggle by running it on the test data (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-06\n",
      "MSE = 1.4809285217339019\n",
      "MSE = 2.3416230648460585\n",
      "MSE = 1.477555249193399\n",
      "MSE = 1.4773868164793784\n",
      "MSE = 1.4767135909964562\n",
      "MSE = 1.4740287750344367\n",
      "MSE = 1.4634188867024385\n",
      "MSE = 1.4230493416308045\n",
      "MSE = 1.0605616058951548\n",
      "MSE = 1.1473576462094621\n",
      "MSE = 1.0033984229096777\n",
      "MSE = 0.9728796310811672\n",
      "MSE = 0.9298791495131971\n",
      "MSE = 0.9207175546048988\n",
      "MSE = 0.9105852772653863\n",
      "MSE = 0.9029128761961507\n",
      "MSE = 0.8986942330396797\n",
      "MSE = 0.8928249969909854\n",
      "MSE = 0.8950410277077057\n",
      "MSE = 0.8909103439647662\n",
      "MSE = 0.8898278751844136\n",
      "MSE = 0.9298014310565974\n",
      "MSE = 0.8897531460656499\n",
      "MSE = 0.8891983576013961\n",
      "MSE = 0.8890042102985608\n",
      "MSE = 0.8887628545967325\n",
      "MSE = 0.8883835164219885\n",
      "MSE = 0.887777246676655\n",
      "MSE = 0.8875345165946327\n",
      "MSE = 0.8873718573540708\n",
      "MSE = 0.8870911379655827\n",
      "MSE = 0.8869402853972562\n",
      "MSE = 0.8868728250640152\n",
      "MSE = 0.8868968118749595\n",
      "MSE = 0.8868700699441681\n",
      "MSE = 0.8868617161719714\n",
      "MSE = 0.8868559572322702\n",
      "MSE = 0.8868453068922804\n",
      "MSE = 0.8868396348537826\n",
      "MSE = 0.8868460500877247\n",
      "MSE = 0.8868348821594247\n",
      "MSE = 0.8867901521930013\n",
      "MSE = 0.8867707583236927\n",
      "MSE = 0.8867506047830691\n",
      "MSE = 0.8867333804002061\n",
      "MSE = 0.8867015209236712\n",
      "MSE = 0.8867059057163283\n",
      "MSE = 0.8867001037053891\n",
      "MSE = 0.8866965958327282\n",
      "MSE = 0.8866961902718138\n",
      "MSE = 0.8866958253739924\n",
      "MSE = 0.8867652434336762\n",
      "MSE = 0.8866963146986949\n",
      "MSE = 0.8866953903960946\n",
      "MSE = 0.8866936169383591\n",
      "MSE = 0.8866914979108993\n",
      "MSE = 0.8866868753979837\n",
      "MSE = 0.8866795255052091\n",
      "MSE = 0.8866647908267925\n",
      "MSE = 0.8868877974375269\n",
      "MSE = 0.8866646427450305\n",
      "MSE = 0.8866611814687858\n",
      "MSE = 0.886660703174706\n",
      "MSE = 0.8866603348832934\n",
      "MSE = 0.8868123589570929\n",
      "MSE = 0.8866605698253177\n",
      "MSE = 0.8866599145222533\n",
      "MSE = 0.8866582069363963\n",
      "MSE = 0.8866551861354174\n",
      "MSE = 0.886650685736915\n",
      "MSE = 0.8866558258685266\n",
      "MSE = 0.8866488823528204\n",
      "MSE = 0.8866448000187962\n",
      "MSE = 0.8870337277987943\n",
      "MSE = 0.8866446948469254\n",
      "MSE = 0.8866410875565552\n",
      "MSE = 0.8866381787054456\n",
      "MSE = 0.8866347822257001\n",
      "MSE = 0.8866294043626631\n",
      "MSE = 0.8866329676830248\n",
      "MSE = 0.8866274335353841\n",
      "MSE = 0.8866246317724746\n",
      "MSE = 0.8866222784399852\n",
      "MSE = 0.8866215394177794\n",
      "MSE = 0.8866273300162751\n",
      "MSE = 0.8866197398471484\n",
      "MSE = 0.8866166120773807\n",
      "MSE = 0.8883441365333214\n",
      "MSE = 0.8866164623248213\n",
      "MSE = 0.8866121717585824\n",
      "MSE = 0.886606648228479\n",
      "MSE = 0.8866030237785087\n",
      "MSE = 0.8866104758854004\n",
      "MSE = 0.8866014871301559\n",
      "MSE = 0.8866005704235355\n",
      "MSE = 0.8866003604956147\n",
      "MSE = 0.8865999561448974\n",
      "MSE = 0.8865964025223099\n",
      "MSE = 0.8865978174163364\n",
      "MSE = 0.8865960278337679\n",
      "MSE = 0.8865947451753309\n",
      "MSE = 0.8865935221282311\n",
      "MSE = 0.8866012204456596\n",
      "MSE = 0.8865932810501547\n",
      "MSE = 0.886593305838453\n",
      "MSE = 0.8865939513681471\n",
      "MSE = 0.8865941186967861\n",
      "MSE = 0.886593943326206\n",
      "MSE = 0.8865961629213754\n",
      "MSE = 0.8865933317446881\n",
      "MSE = 0.8865921512180386\n",
      "MSE = 0.886611227239749\n",
      "MSE = 0.8865934074145587\n",
      "MSE = 0.8865917267628162\n",
      "MSE = 0.886607033119509\n",
      "MSE = 0.886592995636699\n",
      "MSE = 0.8865913717033663\n",
      "MSE = 0.8865913136142357\n",
      "MSE = 0.886591201121203\n",
      "MSE = 0.8865903392579776\n",
      "MSE = 0.8865894120296812\n",
      "MSE = 0.8865881790311593\n",
      "MSE = 0.8865871630505199\n",
      "MSE = 0.8865867430642206\n",
      "MSE = 0.8865869702633645\n",
      "MSE = 0.8866221310415083\n",
      "MSE = 0.8865862764951191\n",
      "MSE = 0.8865862427002373\n",
      "MSE = 0.8865852222468514\n",
      "MSE = 0.8865833019082365\n",
      "MSE = 0.8873151454566358\n",
      "MSE = 0.8865832665348405\n",
      "MSE = 0.8865800134387375\n",
      "MSE = 0.8865775850320526\n",
      "MSE = 0.8865766399409895\n",
      "MSE = 0.8865761498964996\n",
      "MSE = 0.886576014404723\n",
      "MSE = 0.8866126303735966\n",
      "MSE = 0.8865758368277747\n",
      "MSE = 0.8865737439174681\n",
      "MSE = 0.886575286402055\n",
      "MSE = 0.8865756362357712\n",
      "MSE = 0.8865758760705258\n",
      "MSE = 0.8865719227930154\n",
      "MSE = 0.8866116490273617\n",
      "MSE = 0.8865718797355708\n",
      "MSE = 0.8865699456282687\n",
      "MSE = 0.8865951309259693\n",
      "MSE = 0.886570852993265\n",
      "MSE = 0.886568818099353\n",
      "MSE = 0.8865652134965123\n",
      "MSE = 0.886564262793411\n",
      "MSE = 0.8865603797178048\n",
      "MSE = 0.8865786432547089\n",
      "MSE = 0.8865599367287352\n",
      "MSE = 0.886559956109017\n",
      "MSE = 0.8865589537550843\n",
      "9e-05\n",
      "MSE = 1.486786420024875\n",
      "MSE = 2.286868543475607\n",
      "MSE = 1.4775544773469662\n",
      "MSE = 1.4773887813156974\n",
      "MSE = 1.4767264862555707\n",
      "MSE = 1.474085131057442\n",
      "MSE = 1.4636449109403535\n",
      "MSE = 1.4238872412820058\n",
      "MSE = 1.0957917861345003\n",
      "MSE = 1.1005304005918912\n",
      "MSE = 1.057199342962639\n",
      "MSE = 1.0597721622447485\n",
      "MSE = 1.051463727965986\n",
      "MSE = 1.036616635567677\n",
      "MSE = 1.0350216376929429\n",
      "MSE = 1.034398398474339\n",
      "MSE = 1.033411359815965\n",
      "MSE = 1.0336076981843867\n",
      "MSE = 1.0333002280757735\n",
      "MSE = 1.0335356871690198\n",
      "MSE = 1.035510764857318\n",
      "MSE = 1.0335358381350073\n",
      "MSE = 1.0335665679049084\n",
      "MSE = 1.0335754326757536\n",
      "MSE = 1.0335236532822574\n",
      "1e-05\n",
      "MSE = 1.4811534605271366\n",
      "MSE = 2.339588277439953\n",
      "MSE = 1.4775552792741842\n",
      "MSE = 1.477387124080497\n",
      "MSE = 1.4767150070138457\n",
      "MSE = 1.4740345980724272\n",
      "MSE = 1.463441911512248\n",
      "MSE = 1.423134352560154\n",
      "MSE = 1.0609189244303276\n",
      "MSE = 1.1290249262501917\n",
      "MSE = 1.0040355319867391\n",
      "MSE = 0.9775046932942201\n",
      "MSE = 0.9352915890360357\n",
      "MSE = 0.9233787811440974\n",
      "MSE = 0.9158060403578886\n",
      "MSE = 0.909949209854557\n",
      "MSE = 0.9057632904169471\n",
      "MSE = 0.899801501238192\n",
      "MSE = 0.9005091408980421\n",
      "MSE = 0.898502957580323\n",
      "MSE = 0.897441865832409\n",
      "MSE = 0.9199884244335724\n",
      "MSE = 0.8973840833099169\n",
      "MSE = 0.8970909259797245\n",
      "MSE = 0.8970244494166747\n",
      "MSE = 0.8969125613748092\n",
      "MSE = 0.8966165944658833\n",
      "MSE = 0.8961440995777533\n",
      "MSE = 0.8959218479673842\n",
      "MSE = 0.8957760843391492\n",
      "MSE = 0.8955041379893334\n",
      "MSE = 0.895403037208747\n",
      "MSE = 0.8953750436161736\n",
      "MSE = 0.8953776283747682\n",
      "MSE = 0.8953812567164189\n",
      "MSE = 0.8953759435443539\n",
      "MSE = 0.8953604362121447\n",
      "MSE = 0.8953553918222912\n",
      "MSE = 0.8953454546258727\n",
      "MSE = 0.8953445125356824\n",
      "MSE = 0.895349607819556\n",
      "MSE = 0.8953530363022046\n",
      "MSE = 0.8953618100168728\n",
      "MSE = 0.8953599149502306\n",
      "MSE = 0.8953403055845687\n",
      "MSE = 0.8953532606819832\n",
      "MSE = 0.8953470301152174\n",
      "MSE = 0.8953457173673803\n",
      "MSE = 0.8953439090654544\n",
      "MSE = 0.8953432865945969\n",
      "MSE = 0.8953432612129156\n",
      "MSE = 0.8953429364111529\n",
      "MSE = 0.8953418409649865\n",
      "MSE = 0.8953395498087233\n",
      "MSE = 0.8953384069390496\n",
      "MSE = 0.8953353558424897\n",
      "MSE = 0.8953428024009549\n",
      "MSE = 0.8953330842798704\n",
      "MSE = 0.8953320908142783\n",
      "MSE = 0.8953320789161225\n",
      "MSE = 0.8953315923967502\n",
      "MSE = 0.8953329951169179\n",
      "MSE = 0.8953335929591445\n",
      "MSE = 0.8953335771934222\n",
      "MSE = 0.8953325473295255\n",
      "MSE = 0.8953331184142702\n",
      "MSE = 0.8953321882437049\n",
      "MSE = 0.8953295163780258\n",
      "MSE = 0.8953260471088741\n",
      "MSE = 0.8953234044091468\n",
      "MSE = 0.8953216491054423\n",
      "MSE = 0.8953211698242001\n",
      "MSE = 0.8953209711062112\n",
      "MSE = 0.8953206137881939\n",
      "MSE = 0.8953207157568095\n",
      "MSE = 0.8953205693296908\n",
      "MSE = 0.8953201263807953\n",
      "MSE = 0.8953192791800457\n",
      "MSE = 0.8953179906581601\n",
      "MSE = 0.895315404206004\n",
      "MSE = 0.89531162530364\n",
      "MSE = 0.8953059608642412\n",
      "MSE = 0.8952992060459443\n",
      "MSE = 0.8952952522377633\n",
      "MSE = 0.8952980354262178\n",
      "MSE = 0.8952972678618738\n",
      "MSE = 0.8952978455766679\n",
      "MSE = 0.8952981410762304\n",
      "MSE = 0.8952980756010311\n",
      "MSE = 0.8952978377561757\n",
      "MSE = 0.8952953054590973\n",
      "MSE = 0.8955381662880884\n",
      "MSE = 0.8952964699063861\n",
      "MSE = 0.895289783535489\n",
      "MSE = 0.8952882571395075\n",
      "MSE = 0.895286765952469\n",
      "MSE = 0.8952845431934989\n",
      "MSE = 0.8952810738756023\n",
      "MSE = 0.8952798479998817\n",
      "MSE = 0.8952798714571569\n",
      "MSE = 0.8952796578640211\n",
      "MSE = 0.895284060548831\n",
      "MSE = 0.8952803854680563\n",
      "MSE = 0.8952758728793355\n",
      "MSE = 0.895269954502762\n",
      "0.00095\n",
      "MSE = 1.4854498884734442\n",
      "MSE = 2.298526684273314\n",
      "MSE = 1.4775547518891456\n",
      "MSE = 1.477388945018824\n",
      "MSE = 1.476726207244611\n",
      "MSE = 1.474083091463699\n",
      "MSE = 1.4636359933952627\n",
      "MSE = 1.3502084599650273\n",
      "MSE = 1.3518082804551268\n",
      "MSE = 1.3520128533355917\n",
      "MSE = 1.351986656830229\n",
      "0.00089\n",
      "MSE = 1.478064565283354\n",
      "MSE = 2.2430189128285294\n",
      "MSE = 1.4775396599028172\n",
      "MSE = 1.477336032897471\n",
      "MSE = 1.476522264097546\n",
      "MSE = 1.473279016441515\n",
      "MSE = 1.4604952665153565\n",
      "MSE = 1.3454120478335587\n",
      "MSE = 1.3456824856149971\n",
      "MSE = 1.3457671593690888\n",
      "MSE = 1.345696966896155\n",
      "0.0001\n",
      "MSE = 1.478096877672179\n",
      "MSE = 2.2553164603001634\n",
      "MSE = 1.4775413298010447\n",
      "MSE = 1.477340972381583\n",
      "MSE = 1.4765402579098588\n",
      "MSE = 1.4733488433219317\n",
      "MSE = 1.4607662777532304\n",
      "MSE = 1.4133655000052248\n",
      "MSE = 1.1063795502470466\n",
      "MSE = 1.0872751573588626\n",
      "MSE = 1.0747551070287655\n",
      "MSE = 1.0708372468376834\n",
      "MSE = 1.0577620661291969\n",
      "MSE = 1.0490390986454583\n",
      "MSE = 1.0483121409639988\n",
      "MSE = 1.047999171132228\n",
      "MSE = 1.0476119202668936\n",
      "MSE = 1.0472706658642545\n",
      "MSE = 1.0472843413519375\n",
      "MSE = 1.0472242186457152\n",
      "MSE = 1.0475685433049362\n",
      "MSE = 1.047223420372444\n",
      "MSE = 1.0472440344145983\n"
     ]
    }
   ],
   "source": [
    "lamb = [0.000001,0.00009, 0.00001, 0.00095, 0.00089, 0.0001]\n",
    "MSE_train = []\n",
    "MSE_valid = []\n",
    "results_11 = []\n",
    "\n",
    "for i in lamb:\n",
    "    print(i)\n",
    "    result = scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nItems),\n",
    "                             derivative, args = (labels_train, i))\n",
    "    \n",
    "    results_11.append(result)\n",
    "    predictions = []\n",
    "\n",
    "    for d in train:\n",
    "        user,book = d['userID'], d['bookID']\n",
    "        \n",
    "        if book in itemBiases and user in userBiases:\n",
    "             pred1 = alpha + userBiases[user] + itemBiases[book]\n",
    "        else:\n",
    "            pred1 = alpha\n",
    "        \n",
    "        predictions.append(pred1)\n",
    "        \n",
    "    MSE_t = MSE(predictions,labels_train)\n",
    "        \n",
    "    MSE_train.append(MSE_t)\n",
    "        \n",
    "    predictions_validation = []\n",
    "\n",
    "    for d in validation:\n",
    "        user,book = d['userID'], d['bookID']\n",
    "        \n",
    "        if book in itemBiases and user in userBiases:\n",
    "             pred1 = alpha + userBiases[user] + itemBiases[book]\n",
    "        else:\n",
    "            pred1 = alpha\n",
    "        \n",
    "        predictions_validation.append(pred1)\n",
    "    \n",
    "    MSE_pred = MSE(predictions_validation,labels_valid)    \n",
    "    MSE_valid.append(MSE_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For question 11, I decided to improve my lambda by plotting a visual graph to see which lambda is the best. I also corrected my code and did not use the function with gamma which I had mistakenly used in HMW3, this is why I had such a low result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "graph = [0,1,2,3,4,5]\n",
    "\n",
    "plt.plot(graph, MSE_train, color=\"turquoise\", label='Train Mean Standard Error')\n",
    "plt.plot(graph, MSE_valid,color=\"orange\", label='Validation Mean Standard Error')\n",
    "plt.xlabel('Values of Lambda')\n",
    "plt.legend()\n",
    "plt.xticks(graph, lamb)\n",
    "plt.ylabel('Mean Standard Error')\n",
    "plt.suptitle('Mean Standard Error per Lambda values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the graph above the best value for lambda is 0.00001, as it gives us the smallest MSE for both the train and validation sets. Since I now have an accurate lambda, I repeat the steps in question 9 to train the entire dataset using that lambda of 0.00001. This is the mistake I made in hmw#3: I did not rerun the predictions with the best lambda. For simplicity I will re-upload the data and start over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"train_Interactions.csv.gz\"\n",
    "f = gzip.open(path, 'rt', encoding=\"utf8\")\n",
    "header = f.readline()\n",
    "header = header.strip().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset9 = []\n",
    "for line in f:\n",
    "    fields = line.strip().split(',')\n",
    "    d = dict(zip(header, fields))\n",
    "    d['rating'] = int(d['rating'])\n",
    "    dataset9.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerBook = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dataset9:\n",
    "    user,book = d['userID'], d['bookID']\n",
    "    ratingsPerUser[user].append(d)\n",
    "    ratingsPerBook[book].append(d)\n",
    "    rating = d['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingMean = sum([int(d['rating']) for d in train]) / len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(dataset9)\n",
    "nUsers = len(ratingsPerUser)\n",
    "nItems = len(ratingsPerBook)\n",
    "users = list(ratingsPerUser.keys())\n",
    "items = list(ratingsPerBook.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(user, item):\n",
    "    return alpha + userBiases[user] + itemBiases[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = ratingMean\n",
    "userBiases = defaultdict(float)\n",
    "itemBiases = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global itemBiases\n",
    "    alpha = theta[0]\n",
    "    userBiases = dict(zip(users, theta[1:nUsers+1]))\n",
    "    itemBiases = dict(zip(items, theta[1+nUsers:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(d['userID'], d['bookID']) for d in dataset9]\n",
    "    cost = MSE(predictions, labels)\n",
    "    print(\"MSE = \" + str(cost))\n",
    "    for u in userBiases:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "    for i in itemBiases:\n",
    "        cost += lamb*itemBiases[i]**2\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(dataset9)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    for d in dataset9:\n",
    "        u,b = d['userID'], d['bookID']\n",
    "        pred = prediction(u, b)\n",
    "        diff = pred - int(d['rating'])\n",
    "        dalpha += 2/N*diff\n",
    "        dUserBiases[u] += 2/N*diff\n",
    "        dItemBiases[b] += 2/N*diff\n",
    "    for u in userBiases:\n",
    "        dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "    for b in itemBiases:\n",
    "        dItemBiases[b] += 2*lamb*itemBiases[b]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[b] for b in items]\n",
    "    return numpy.array(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = 0.00001\n",
    "y = [int(d['rating']) for d in dataset9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nItems),\n",
    "                             derivative, args = (y, final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Rating.txt\", 'w')\n",
    "prediction_rating = []\n",
    "\n",
    "for l in open(\"pairs_Rating.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split('-')\n",
    "    if u in userBiases and b in itemBiases:\n",
    "        \n",
    "        pred = alpha + userBiases[u] + itemBiases[b]\n",
    "        predictions.write(u + '-' + b + ',' + str(pred) + '\\n')\n",
    "    else:\n",
    "        pred = int(alpha)\n",
    "        predictions.write(u + '-' + b + ',' + str(pred) + '\\n')\n",
    "        \n",
    "    prediction_rating.append(pred)\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new score is 1.138034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlast = pd.read_csv('pairs_Rating.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlast['prediction'] = v_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(v_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv = testlast.to_csv ('predictions_Rating.csv', index = None, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
